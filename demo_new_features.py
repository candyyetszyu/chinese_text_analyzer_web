#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chinese Text Analyzer - New Features Demo
æ¼”ç¤ºæ–°åŠŸèƒ½ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ã€é«˜ç´šè¦–è¦ºåŒ–ã€ä»»å‹™éšŠåˆ—ã€GPUåŠ é€Ÿã€å¤šæ ¼å¼æ”¯æŒ
"""

import os
import sys
import time
import json
from pathlib import Path

# Add src to path
sys.path.append('src')

def demo_text_similarity():
    """æ¼”ç¤ºæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ææ¼”ç¤º")
    print("="*60)
    
    try:
        from src.core.similarity import TextSimilarityAnalyzer
        
        # ç¤ºä¾‹æ–‡æœ¬
        texts = [
            "ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œé™½å…‰æ˜åªšï¼Œé©åˆå‡ºé–€æ•£æ­¥ã€‚",
            "ä»Šæ—¥å¤©æ°£æ™´æœ—ï¼Œé™½å…‰ç‡¦çˆ›ï¼Œå¾ˆé©åˆå¤–å‡ºé‹å‹•ã€‚", 
            "æ˜¨å¤©ä¸‹é›¨äº†ï¼Œè·¯ä¸Šå¾ˆæ¿•æ»‘ï¼Œå‡ºè¡Œä¸ä¾¿ã€‚",
            "æ˜¨æ—¥ä¸‹äº†å¤§é›¨ï¼Œé“è·¯æ¿•æ»‘ï¼Œäº¤é€šå—é˜»ã€‚",
            "æˆ‘å–œæ­¡åƒä¸­åœ‹èœï¼Œç‰¹åˆ¥æ˜¯å·èœå’Œç²µèœã€‚"
        ]
        
        labels = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", "æ–‡æœ¬4", "æ–‡æœ¬5"]
        
        print("ğŸ“ åˆ†ææ–‡æœ¬:")
        for i, text in enumerate(texts):
            print(f"  {labels[i]}: {text}")
        
        # åˆå§‹åŒ–ç›¸ä¼¼åº¦åˆ†æå™¨
        print("\nğŸš€ åˆå§‹åŒ–ç›¸ä¼¼åº¦åˆ†æå™¨...")
        similarity_analyzer = TextSimilarityAnalyzer(use_gpu=False)
        
        # ç¶œåˆç›¸ä¼¼åº¦åˆ†æ
        print("\nğŸ“Š åŸ·è¡Œç¶œåˆç›¸ä¼¼åº¦åˆ†æ...")
        results = similarity_analyzer.comprehensive_similarity_analysis(texts, labels)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å…±åˆ†æ {results['text_count']} å€‹æ–‡æœ¬")
        print(f"ğŸ“ˆ ç”Ÿæˆ {len(results['pairwise_comparisons'])} å€‹å…©å…©æ¯”è¼ƒçµæœ")
        
        # é¡¯ç¤ºæœ€ç›¸ä¼¼çš„æ–‡æœ¬å°
        print("\nğŸ† æœ€ç›¸ä¼¼çš„æ–‡æœ¬å°ï¼ˆèªç¾©ç›¸ä¼¼åº¦ï¼‰:")
        similar_pairs = similarity_analyzer.find_most_similar_pairs(
            texts, labels, method='semantic', top_k=3
        )
        
        for i, pair in enumerate(similar_pairs):
            print(f"  {i+1}. {pair['text1_label']} â†” {pair['text2_label']}")
            print(f"     ç›¸ä¼¼åº¦: {pair['similarity']:.3f}")
        
        # èšé¡ç›¸ä¼¼æ–‡æœ¬
        print("\nğŸ¯ æ–‡æœ¬èšé¡çµæœï¼ˆé–¾å€¼=0.7ï¼‰:")
        clusters = similarity_analyzer.cluster_similar_texts(
            texts, labels, method='semantic', threshold=0.7
        )
        
        for i, cluster in enumerate(clusters):
            print(f"  é›†ç¾¤ {i+1}: {[item['label'] for item in cluster]}")
        
        return results
        
    except ImportError as e:
        print(f"âŒ æ–‡æœ¬ç›¸ä¼¼åº¦åŠŸèƒ½ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ è«‹å®‰è£: pip install sentence-transformers torch")
        return None

def demo_advanced_visualization(similarity_results=None):
    """æ¼”ç¤ºé«˜ç´šè¦–è¦ºåŒ–åŠŸèƒ½"""
    print("\n" + "="*60) 
    print("ğŸ“Š é«˜ç´šè¦–è¦ºåŒ–æ¼”ç¤º")
    print("="*60)
    
    try:
        from src.core.advanced_visualization import AdvancedVisualizer
        import numpy as np
        
        print("ğŸš€ åˆå§‹åŒ–é«˜ç´šè¦–è¦ºåŒ–å™¨...")
        adv_viz = AdvancedVisualizer()
        
        # ç¤ºä¾‹è©é »æ•¸æ“š
        word_freq = {
            'å¤©æ°£': 15, 'é™½å…‰': 12, 'æ•£æ­¥': 8, 'é‹å‹•': 10,
            'ä¸‹é›¨': 6, 'é“è·¯': 5, 'äº¤é€š': 4, 'ä¸­åœ‹èœ': 7,
            'å·èœ': 3, 'ç²µèœ': 3, 'ä»Šå¤©': 8, 'æ˜¨å¤©': 6,
            'å‡ºé–€': 5, 'å¤–å‡º': 4, 'æ¿•æ»‘': 3, 'æ™´æœ—': 6
        }
        
        print("ğŸ“ˆ ç”Ÿæˆè©é »æ¨¹ç‹€åœ–...")
        if adv_viz.plotly_available:
            treemap_fig = adv_viz.plot_word_frequency_treemap(word_freq)
            treemap_fig.write_html("visualizations/treemap.html")
            print("âœ… æ¨¹ç‹€åœ–å·²ä¿å­˜åˆ°: visualizations/treemap.html")
        else:
            print("âŒ Plotlyä¸å¯ç”¨ï¼Œè·³éæ¨¹ç‹€åœ–ç”Ÿæˆ")
        
        # ç›¸ä¼¼åº¦ç†±åŠ›åœ–
        if similarity_results and adv_viz.plotly_available:
            print("\nğŸ”¥ ç”Ÿæˆç›¸ä¼¼åº¦ç†±åŠ›åœ–...")
            similarity_matrix = np.array(similarity_results['similarities']['semantic'])
            labels = similarity_results['labels']
            
            # éœæ…‹ç†±åŠ›åœ–
            os.makedirs("visualizations", exist_ok=True)
            adv_viz.plot_similarity_heatmap(
                similarity_matrix, labels, 
                save_path="visualizations/similarity_heatmap.png"
            )
            print("âœ… éœæ…‹ç†±åŠ›åœ–å·²ä¿å­˜åˆ°: visualizations/similarity_heatmap.png")
            
            # äº¤äº’å¼ç†±åŠ›åœ–
            interactive_heatmap = adv_viz.plot_interactive_similarity_heatmap(
                similarity_matrix, labels
            )
            interactive_heatmap.write_html("visualizations/interactive_heatmap.html")
            print("âœ… äº¤äº’å¼ç†±åŠ›åœ–å·²ä¿å­˜åˆ°: visualizations/interactive_heatmap.html")
            
            # ç¶²çµ¡åœ–
            print("\nğŸŒ ç”Ÿæˆæ–‡æœ¬ç›¸ä¼¼åº¦ç¶²çµ¡åœ–...")
            if adv_viz.networkx_available:
                adv_viz.plot_text_network(
                    similarity_matrix, labels, threshold=0.5,
                    save_path="visualizations/similarity_network.png"
                )
                print("âœ… ç¶²çµ¡åœ–å·²ä¿å­˜åˆ°: visualizations/similarity_network.png")
                
                # äº¤äº’å¼ç¶²çµ¡åœ–
                interactive_network = adv_viz.plot_interactive_network(
                    similarity_matrix, labels, threshold=0.5
                )
                interactive_network.write_html("visualizations/interactive_network.html")
                print("âœ… äº¤äº’å¼ç¶²çµ¡åœ–å·²ä¿å­˜åˆ°: visualizations/interactive_network.html")
            else:
                print("âŒ NetworkXä¸å¯ç”¨ï¼Œè·³éç¶²çµ¡åœ–ç”Ÿæˆ")
        
        # äº¤äº’å¼è©é »åˆ†æ
        if adv_viz.plotly_available:
            print("\nğŸ“Š ç”Ÿæˆäº¤äº’å¼è©é »åˆ†æ...")
            interactive_analysis = adv_viz.plot_interactive_word_cloud_data(word_freq)
            interactive_analysis.write_html("visualizations/interactive_word_analysis.html")
            print("âœ… äº¤äº’å¼è©é »åˆ†æå·²ä¿å­˜åˆ°: visualizations/interactive_word_analysis.html")
        
        # å‰µå»ºå„€è¡¨æ¿
        if adv_viz.plotly_available:
            print("\nğŸ“‹ å‰µå»ºç¶œåˆå„€è¡¨æ¿...")
            figures = {
                "è©é »æ¨¹ç‹€åœ–": adv_viz.plot_word_frequency_treemap(word_freq),
                "äº¤äº’å¼è©é »åˆ†æ": adv_viz.plot_interactive_word_cloud_data(word_freq)
            }
            
            if similarity_results:
                similarity_matrix = np.array(similarity_results['similarities']['semantic'])
                labels = similarity_results['labels']
                figures["ç›¸ä¼¼åº¦ç†±åŠ›åœ–"] = adv_viz.plot_interactive_similarity_heatmap(
                    similarity_matrix, labels
                )
                if adv_viz.networkx_available:
                    figures["æ–‡æœ¬ç¶²çµ¡åœ–"] = adv_viz.plot_interactive_network(
                        similarity_matrix, labels
                    )
            
            dashboard_path = adv_viz.create_dashboard_html(
                figures, "visualizations/dashboard.html"
            )
            print(f"âœ… ç¶œåˆå„€è¡¨æ¿å·²ä¿å­˜åˆ°: {dashboard_path}")
        
    except ImportError as e:
        print(f"âŒ é«˜ç´šè¦–è¦ºåŒ–åŠŸèƒ½ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ è«‹å®‰è£: pip install plotly networkx")

def demo_extended_file_support():
    """æ¼”ç¤ºæ“´å±•æ–‡ä»¶æ ¼å¼æ”¯æŒ"""
    print("\n" + "="*60)
    print("ğŸ“ æ“´å±•æ–‡ä»¶æ ¼å¼æ”¯æŒæ¼”ç¤º") 
    print("="*60)
    
    try:
        from src.utils.file_parsers import ExtendedFileParser
        
        parser = ExtendedFileParser()
        
        print("ğŸš€ æ–‡ä»¶è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“‹ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:")
        for ext in parser.get_supported_extensions():
            print(f"  âœ“ .{ext}")
        
        print("\nğŸ”§ åŠŸèƒ½å¯ç”¨æ€§:")
        for feature, available in parser.capabilities.items():
            status = "âœ…" if available else "âŒ"
            print(f"  {status} {feature}")
        
        # å‰µå»ºç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¸¬è©¦
        test_dir = Path("test_files")
        test_dir.mkdir(exist_ok=True)
        
        # å‰µå»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
        sample_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡ä»¶ã€‚\nåŒ…å«ä¸­æ–‡æ–‡æœ¬å…§å®¹ã€‚\nç”¨æ–¼æ¼”ç¤ºæ–‡ä»¶è§£æåŠŸèƒ½ã€‚"
        
        test_files = []
        
        # TXTæ–‡ä»¶
        txt_file = test_dir / "sample.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        test_files.append(str(txt_file))
        
        # Markdownæ–‡ä»¶
        md_file = test_dir / "sample.md"
        md_content = f"""# æ¸¬è©¦æ–‡æª”
        
## æ¦‚è¿°
{sample_text}

**é‡é»**: é€™æ˜¯markdownæ ¼å¼çš„æ–‡ä»¶ã€‚

- åˆ—è¡¨é …ç›®1
- åˆ—è¡¨é …ç›®2
"""
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        test_files.append(str(md_file))
        
        # JSONæ–‡ä»¶
        json_file = test_dir / "sample.json"
        json_content = {
            "title": "æ¸¬è©¦æ•¸æ“š",
            "content": sample_text,
            "metadata": {
                "author": "ç³»çµ±",
                "date": "2024-01-01"
            }
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_content, f, ensure_ascii=False, indent=2)
        test_files.append(str(json_file))
        
        print(f"\nğŸ“‚ å‰µå»ºäº† {len(test_files)} å€‹æ¸¬è©¦æ–‡ä»¶")
        
        # æ‰¹é‡è§£ææ–‡ä»¶
        print("\nğŸ”„ æ‰¹é‡è§£ææ–‡ä»¶...")
        results = parser.batch_parse_files(test_files)
        
        for file_path, result in results.items():
            file_name = Path(file_path).name
            if 'error' in result.get('metadata', {}):
                print(f"âŒ {file_name}: {result['metadata']['error']}")
            else:
                content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                print(f"âœ… {file_name}: {result['metadata']['file_type']} - {len(result['content'])} å­—ç¬¦")
                print(f"   é è¦½: {content_preview}")
        
        # æ¼”ç¤ºURLè§£æï¼ˆå¦‚æœç¶²çµ¡å¯ç”¨ï¼‰
        print("\nğŸŒ æ¸¬è©¦ç¶²é è§£æ...")
        if parser.capabilities['web_scraping']:
            try:
                # è§£æä¸€å€‹ç°¡å–®çš„ç¶²é 
                url_result = parser.parse_url("https://httpbin.org/html")
                print(f"âœ… ç¶²é è§£ææˆåŠŸ: {len(url_result['content'])} å­—ç¬¦")
                print(f"   æ¨™é¡Œ: {url_result['metadata'].get('title', 'N/A')}")
            except Exception as e:
                print(f"âŒ ç¶²é è§£æå¤±æ•—: {e}")
        
        return results
        
    except ImportError as e:
        print(f"âŒ æ“´å±•æ–‡ä»¶æ ¼å¼åŠŸèƒ½ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ è«‹å®‰è£: pip install PyPDF2 python-docx beautifulsoup4 pdfplumber")
        return None

def demo_task_queue():
    """æ¼”ç¤ºä»»å‹™éšŠåˆ—ç³»çµ±"""
    print("\n" + "="*60)
    print("âš™ï¸ ä»»å‹™éšŠåˆ—ç³»çµ±æ¼”ç¤º")
    print("="*60)
    
    try:
        from src.core.task_queue import TaskQueue, TaskType, TaskStatus
        
        print("ğŸš€ åˆå§‹åŒ–ä»»å‹™éšŠåˆ—...")
        task_queue = TaskQueue(enable_celery=False)  # ä½¿ç”¨æœ¬åœ°è™•ç†é€²è¡Œæ¼”ç¤º
        
        # å‰µå»ºæ–‡æœ¬åˆ†æä»»å‹™
        print("\nğŸ“ å‰µå»ºæ–‡æœ¬åˆ†æä»»å‹™...")
        analysis_params = {
            'text': 'é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ï¼Œç”¨ä¾†æ¼”ç¤ºä»»å‹™éšŠåˆ—ç³»çµ±çš„æ–‡æœ¬åˆ†æåŠŸèƒ½ã€‚æˆ‘å€‘å°‡åˆ†æé€™æ®µæ–‡æœ¬çš„è©é »ã€æƒ…æ„Ÿå’Œé—œéµè©ã€‚',
            'options': {
                'include_sentiment': True,
                'include_keywords': True,
                'include_entities': True,
                'include_ngrams': True
            }
        }
        
        task_id = task_queue.create_task(
            TaskType.TEXT_ANALYSIS, 
            analysis_params,
            estimated_duration=30
        )
        print(f"âœ… ä»»å‹™å·²å‰µå»ºï¼ŒID: {task_id}")
        
        # ç›£æ§ä»»å‹™é€²åº¦
        print("\nâ³ ç›£æ§ä»»å‹™é€²åº¦...")
        for i in range(10):  # æœ€å¤šç­‰å¾…10ç§’
            task = task_queue.get_task_status(task_id)
            if task:
                print(f"  ç‹€æ…‹: {task.status.value} | é€²åº¦: {task.progress}%")
                
                if task.status == TaskStatus.SUCCESS:
                    print("âœ… ä»»å‹™å®Œæˆï¼")
                    if task.result:
                        print(f"   è©é »çµ±è¨ˆ: {len(task.result.get('word_frequency', {}))} å€‹è©")
                        print(f"   æƒ…æ„Ÿåˆ†æ: {task.result.get('sentiment', {}).get('sentiment_label', 'N/A')}")
                        print(f"   é—œéµè©: {len(task.result.get('keywords', []))} å€‹")
                    break
                elif task.status == TaskStatus.FAILURE:
                    print(f"âŒ ä»»å‹™å¤±æ•—: {task.error_message}")
                    break
            
            time.sleep(1)
        
        # å‰µå»ºç›¸ä¼¼åº¦åˆ†æä»»å‹™
        print("\nğŸ” å‰µå»ºç›¸ä¼¼åº¦åˆ†æä»»å‹™...")
        similarity_params = {
            'texts': [
                'ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œé©åˆå‡ºå»ç©ã€‚',
                'ä»Šæ—¥å¤©æ°£æ™´æœ—ï¼Œå¾ˆé©åˆæˆ¶å¤–æ´»å‹•ã€‚',
                'æ˜¨å¤©ä¸‹é›¨äº†ï¼Œè·¯ä¸Šå¾ˆæ¿•ã€‚'
            ],
            'labels': ['æ–‡æœ¬A', 'æ–‡æœ¬B', 'æ–‡æœ¬C'],
            'method': 'tfidf'  # ä½¿ç”¨TF-IDFä»¥é¿å…éœ€è¦å¤§å‹æ¨¡å‹
        }
        
        similarity_task_id = task_queue.create_task(
            TaskType.SIMILARITY_ANALYSIS,
            similarity_params,
            estimated_duration=20
        )
        print(f"âœ… ç›¸ä¼¼åº¦åˆ†æä»»å‹™å·²å‰µå»ºï¼ŒID: {similarity_task_id}")
        
        # åˆ—å‡ºæ‰€æœ‰ä»»å‹™
        print("\nğŸ“‹ ç•¶å‰ä»»å‹™åˆ—è¡¨:")
        tasks = task_queue.list_tasks(limit=10)
        for task in tasks:
            duration = ""
            if task.completed_at and task.started_at:
                duration = f" ({(task.completed_at - task.started_at).total_seconds():.1f}s)"
            print(f"  {task.id[:8]}... | {task.type.value} | {task.status.value}{duration}")
        
        # ç­‰å¾…ç›¸ä¼¼åº¦ä»»å‹™å®Œæˆ
        print(f"\nâ³ ç­‰å¾…ç›¸ä¼¼åº¦åˆ†æä»»å‹™å®Œæˆ...")
        for i in range(10):
            task = task_queue.get_task_status(similarity_task_id)
            if task and task.status in [TaskStatus.SUCCESS, TaskStatus.FAILURE]:
                if task.status == TaskStatus.SUCCESS:
                    print("âœ… ç›¸ä¼¼åº¦åˆ†æå®Œæˆï¼")
                    if task.result:
                        pairwise_count = len(task.result.get('pairwise_comparisons', []))
                        print(f"   ç”Ÿæˆäº† {pairwise_count} å€‹å…©å…©æ¯”è¼ƒçµæœ")
                else:
                    print(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±æ•—: {task.error_message}")
                break
            time.sleep(1)
        
        return task_queue
        
    except ImportError as e:
        print(f"âŒ ä»»å‹™éšŠåˆ—åŠŸèƒ½ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ è«‹å®‰è£: pip install celery redis")
        return None

def demo_gpu_acceleration():
    """æ¼”ç¤ºGPUåŠ é€ŸåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸš€ GPUåŠ é€ŸåŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    try:
        # æª¢æŸ¥GPUå¯ç”¨æ€§
        try:
            import torch
            gpu_available = torch.cuda.is_available()
            device_count = torch.cuda.device_count()
            
            print(f"ğŸ” CUDAå¯ç”¨æ€§: {'âœ…' if gpu_available else 'âŒ'}")
            if gpu_available:
                print(f"ğŸ“Š GPUè¨­å‚™æ•¸é‡: {device_count}")
                for i in range(device_count):
                    gpu_name = torch.cuda.get_device_name(i)
                    print(f"  GPU {i}: {gpu_name}")
            else:
                print("ğŸ’¡ æœªæª¢æ¸¬åˆ°CUDA GPUï¼Œå°‡ä½¿ç”¨CPUè™•ç†")
            
        except ImportError:
            print("âŒ PyTorchæœªå®‰è£ï¼Œç„¡æ³•æª¢æŸ¥GPUç‹€æ…‹")
            gpu_available = False
        
        # æ¼”ç¤ºç›¸ä¼¼åº¦åˆ†æçš„GPUåŠ é€Ÿ
        print("\nğŸ” æ¸¬è©¦ç›¸ä¼¼åº¦åˆ†æGPUåŠ é€Ÿ...")
        from src.core.similarity import TextSimilarityAnalyzer
        
        # æ¸¬è©¦æ–‡æœ¬
        test_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºèƒ½çš„ä»»å‹™çš„ç³»çµ±ã€‚",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹å­é ˜åŸŸï¼Œå°ˆæ³¨æ–¼é–‹ç™¼èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’çš„ç®—æ³•ã€‚",
            "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²çµ¡ä¾†è™•ç†è¤‡é›œçš„æ•¸æ“šæ¨¡å¼ã€‚",
            "è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹é ˜åŸŸï¼Œå°ˆé–€è™•ç†äººé¡èªè¨€çš„ç†è§£å’Œç”Ÿæˆã€‚",
            "è¨ˆç®—æ©Ÿè¦–è¦ºæ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå°ˆæ³¨æ–¼è®“è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£å’Œåˆ†æè¦–è¦ºä¿¡æ¯ã€‚"
        ]
        
        # CPUè™•ç†
        print("\nâš¡ CPUè™•ç†æ¸¬è©¦...")
        start_time = time.time()
        cpu_analyzer = TextSimilarityAnalyzer(use_gpu=False)
        cpu_results = cpu_analyzer.comprehensive_similarity_analysis(test_texts)
        cpu_time = time.time() - start_time
        print(f"âœ… CPUè™•ç†å®Œæˆï¼Œè€—æ™‚: {cpu_time:.2f}ç§’")
        
        # GPUè™•ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if gpu_available:
            print("\nğŸš€ GPUè™•ç†æ¸¬è©¦...")
            start_time = time.time()
            gpu_analyzer = TextSimilarityAnalyzer(use_gpu=True)
            gpu_results = gpu_analyzer.comprehensive_similarity_analysis(test_texts)
            gpu_time = time.time() - start_time
            print(f"âœ… GPUè™•ç†å®Œæˆï¼Œè€—æ™‚: {gpu_time:.2f}ç§’")
            
            if cpu_time > 0 and gpu_time > 0:
                speedup = cpu_time / gpu_time
                print(f"ğŸƒâ€â™‚ï¸ GPUåŠ é€Ÿå€æ•¸: {speedup:.2f}x")
        else:
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œè·³éGPUæ¸¬è©¦")
            
        # æ¸¬è©¦æ‰¹é‡è™•ç†çš„æ€§èƒ½å·®ç•°
        print("\nğŸ“Š æ‰¹é‡è™•ç†æ€§èƒ½å°æ¯”...")
        larger_texts = test_texts * 3  # å¢åŠ æ–‡æœ¬æ•¸é‡
        
        print(f"ğŸ“ æ¸¬è©¦æ–‡æœ¬æ•¸é‡: {len(larger_texts)}")
        
        # CPUæ‰¹é‡è™•ç†
        start_time = time.time()
        cpu_analyzer.comprehensive_similarity_analysis(larger_texts)
        cpu_batch_time = time.time() - start_time
        print(f"âš¡ CPUæ‰¹é‡è™•ç†è€—æ™‚: {cpu_batch_time:.2f}ç§’")
        
        if gpu_available:
            # GPUæ‰¹é‡è™•ç†
            start_time = time.time()
            gpu_analyzer.comprehensive_similarity_analysis(larger_texts)
            gpu_batch_time = time.time() - start_time
            print(f"ğŸš€ GPUæ‰¹é‡è™•ç†è€—æ™‚: {gpu_batch_time:.2f}ç§’")
            
            if cpu_batch_time > 0 and gpu_batch_time > 0:
                batch_speedup = cpu_batch_time / gpu_batch_time
                print(f"ğŸƒâ€â™‚ï¸ æ‰¹é‡è™•ç†GPUåŠ é€Ÿå€æ•¸: {batch_speedup:.2f}x")
        
    except ImportError as e:
        print(f"âŒ GPUåŠ é€ŸåŠŸèƒ½ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ è«‹å®‰è£: pip install torch sentence-transformers")
    except Exception as e:
        print(f"âŒ GPUæ¸¬è©¦éç¨‹ä¸­å‡ºéŒ¯: {e}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•¸"""
    print("ğŸŒŸ Chinese Text Analyzer - æ–°åŠŸèƒ½æ¼”ç¤º")
    print("ğŸ”§ å±•ç¤ºæ–‡æœ¬ç›¸ä¼¼åº¦ã€é«˜ç´šè¦–è¦ºåŒ–ã€ä»»å‹™éšŠåˆ—ã€GPUåŠ é€Ÿã€å¤šæ ¼å¼æ”¯æŒ")
    print("ğŸ“… " + "="*80)
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs("visualizations", exist_ok=True)
    os.makedirs("test_files", exist_ok=True)
    
    # ä¾æ¬¡æ¼”ç¤ºå„å€‹åŠŸèƒ½
    similarity_results = demo_text_similarity()
    demo_advanced_visualization(similarity_results)
    demo_extended_file_support()
    demo_task_queue()
    demo_gpu_acceleration()
    
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰æ–°åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   ğŸ“Š visualizations/ - è¦–è¦ºåŒ–æ–‡ä»¶")
    print("   ğŸ“‚ test_files/ - æ¸¬è©¦æ–‡ä»¶")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("   1. åœ¨ç€è¦½å™¨ä¸­æ‰“é–‹ visualizations/dashboard.html æŸ¥çœ‹äº¤äº’å¼å„€è¡¨æ¿")
    print("   2. å®‰è£å®Œæ•´ä¾è³´å¾Œå¯ç²å¾—æ›´å¤šåŠŸèƒ½")
    print("   3. é…ç½®Rediså’ŒCeleryå¯å•Ÿç”¨åˆ†ä½ˆå¼ä»»å‹™è™•ç†")
    print("   4. å®‰è£CUDAå’ŒPyTorchå¯å•Ÿç”¨GPUåŠ é€Ÿ")

if __name__ == "__main__":
    main() 